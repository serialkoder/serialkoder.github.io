---
layout: post
title: "CDN Resilience, Security & Observability (System Design Deep Dive)"
tags: [system-design]
---

## Global Resilience Model

A CDN’s global resilience relies on quickly detecting and isolating failures in its Points of Presence (PoPs). **Health checks** continuously probe each PoP or regional endpoint. If a PoP becomes unhealthy (high error rates, no heartbeat), the system **automatically “fails out”** that PoP – traffic is redirected to other PoPs until it recovers. Conversely, once health checks pass again, the PoP is **failed back in** to service. To avoid flapping, CDNs use circuit-breaker logic: after consecutive failures, route away for a short interval (e.g. 1–2 minutes) before retrying. **DNS routing** often underpins this failover; *low TTLs* on DNS records ensure clients quickly get updated endpoints. For example, AWS recommends DNS TTLs around **60 seconds or lower** to enable timely rerouting to healthy endpoints. With fast health-check intervals (e.g. 10s) and a low failover threshold, an outage can trigger global failover in well under two minutes. This minimizes brown-outs by **yanking failing regions quickly**. Overall, the CDN’s global load balancer acts as a smart traffic cop: performing frequent end-to-end probes, and switching traffic away from unhealthy PoPs to maintain availability.

## Multi-Region and Multi-CDN Strategies

For high availability and performance, many architectures deploy **multi-region** PoPs or even **multi-CDN** setups. A multi-region CDN might serve active-active from several geographic regions and gracefully degrade if one region fails (active-passive failover). Multi-CDN takes it further, using multiple providers to add redundancy and optimize performance/cost. An **intelligent steering layer** (often DNS-based or load-balancer based) directs users to the best endpoint. Modern solutions leverage **Real User Monitoring (RUM)** for steering decisions: by collecting real client latency data, the system can route each user to whichever CDN/region currently offers the lowest latency or highest availability. Importantly, this can balance **latency vs. cost SLAs**. For example, the steering policy might send most traffic to the fastest CDN, but if another CDN is slightly slower *and* significantly cheaper, it might route a portion of traffic there to control costs – as long as latency stays within SLA. This **performance-cost optimization** ensures users get good experience without breaking the budget. In practice, multi-CDN routing can use weighted DNS, geolocation, or dynamic RTT measurements. If one CDN has an outage, traffic is automatically shifted to others (graceful degradation). Real-user measurements and health checks drive these decisions so that failover is nearly seamless. Overall, a multi-CDN strategy improves reliability and global performance by not relying on a single network, at the expense of added complexity in routing and monitoring.

## DDoS Mitigation Pipeline

Modern CDNs include a layered **DDoS mitigation pipeline** to absorb and deflect attacks. It starts with **anycast** networking: the CDN advertises the same IP globally so that attack traffic is distributed across many PoPs. This *dilutes* a volumetric attack’s impact by leveraging the CDN’s entire network capacity. Edge routers and scrubbers then perform **traffic filtering**. At the network layer, CDNs use tactics like SYN flood protection with **SYN cookies** – only allocating resources for connections that complete the TCP handshake. This counters botnets sending floods of half-open connections. They also do **connection tracking and rate-limiting** per client IP. By monitoring connection counts and request rates, the CDN can flag anomalies and throttle or block aggressive clients before they overwhelm servers. If attack volume grows, traffic can be rerouted to dedicated **scrubbing centers** – infrastructure with massive bandwidth and specialized filters. These centers use packet inspection and filtering to **cleanse** malicious traffic and forward only legitimate traffic to the origin. Overall, the pipeline typically has multiple stages: anycast absorption at the edge, network-level filtering (blocking common attack patterns, geo-blocking), protocol mitigations (SYN cookies for TCP, rate limits on UDP/ICMP), and application-layer bot detection. The goal is to drop attack traffic as far from the origin as possible, keeping the origin and core network safe. For instance, anycast can disperse a 1 Tbps attack across 100+ PoPs so that each sees a manageable 10 Gbps, while **scrubbing centers** remove the malicious packets and pass on clean traffic. This multi-layered defense is often augmented by services like **AWS Shield Advanced** (for always-on network detection and automated mitigation) as an extra shield in front of the CDN.

## Web Application Firewall and Bot Management

Above the network layer, CDNs deploy a **Web Application Firewall (WAF)** to guard against layer 7 attacks and malicious bots. A WAF uses **rule-based engines** to inspect HTTP(S) traffic. It can block common exploits (SQL injection, XSS, etc.) via managed rule sets (OWASP Core Rules) or custom rules. For instance, rules might match patterns in URLs, headers, or request bodies to identify attacks. **Rate-limiting** rules are also crucial: these detect when a client IP or token is making too many requests and either throttle or block that client. This helps mitigate bots performing credential stuffing or scraping. Advanced bot management goes beyond simple rate limits. CDNs now fingerprint clients using methods like **JA3 TLS fingerprints** – which create a hash of a client’s TLS handshake characteristics. Many bots and automation tools have distinctive TLS signatures. By checking the JA3 fingerprint, the WAF can recognize known bad clients or non-human browsers. For example, AWS WAF and Cloudflare can block or challenge requests with a TLS fingerprint that matches a known botnet or malicious crawler. Additionally, CDNs deploy **JavaScript challenges or CAPTCHAs** to separate bots from humans. A **challenge** (like AWS WAF’s Challenge action) forces the client to solve a computational puzzle invisibly – real browsers handle it, but simplistic bots fail. A **CAPTCHA** is a visible challenge (identifying images, etc.) that ensures a human is present. These steps are triggered for suspicious traffic, such as a client with a high request rate or an unrecognized user-agent. By deploying **multi-step bot detections** – combining IP reputation, behavior analysis (e.g. mouse movements via JS), TLS fingerprints, and challenge-response tests – the CDN can **mitigate malicious bots** while letting legitimate users through. Vendors like AWS and Cloudflare offer **managed bot protection** add-ons that integrate these techniques out of the box, and allow custom rules to fine-tune the detection.

## Transport Security (TLS) at the Edge

CDNs terminate TLS at the edge, so robust **transport security** features are vital. Modern CDN edges support **TLS 1.3**, which brings both security and performance improvements. TLS 1.3 uses only strong ciphers and provides forward secrecy, and it cuts the handshake latency in half compared to TLS 1.2. In fact, TLS 1.3 requires only one round-trip handshake (or zero for resumed sessions) between client and server, shaving significant milliseconds off the connection setup. This is especially beneficial in high-latency mobile networks. CDNs also often enable **0-RTT resumption** with TLS 1.3, allowing repeat visitors to send data immediately without any handshake delay. On the origin side, CDN nodes establish secure **TLS (or mTLS) connections to the origin**. In high-security setups, **mutual TLS** is used: the CDN presents a client certificate to the origin, and the origin validates it, ensuring only the CDN’s servers can pull content. Likewise, the CDN verifies the origin’s cert. This mutual authentication stops malicious actors from bypassing the CDN to hit the origin directly. For example, Cloudflare’s *Authenticated Origin Pulls* and AWS CloudFront’s origin authentication use client certificates to achieve this. Key management is another aspect: CDNs **rotate TLS certificates and keys** regularly. Many provide automatic certificate renewal (e.g. Let’s Encrypt integration or proprietary autorotation) so that edge certificates are always up-to-date and short-lived. Azure Front Door, for instance, auto-rotates its managed TLS certs within 45–90 days of issuance. CDNs also improve client-side performance with **OCSP stapling**. Instead of clients separately contacting a CA to check certificate revocation status, the CDN edge **staples** a recent OCSP response to the TLS handshake. This cuts out an external network call and speeds up secure connections. Most CDN platforms support OCSP stapling by default. Together, these measures (TLS 1.3, mTLS to origin, frequent key rotation, and OCSP stapling) ensure that content delivery is not only fast but also maintains strong encryption and trust at every hop.

## Content Authorization and Secure Access

Beyond transport-level security, CDNs provide **content authorization** mechanisms to ensure only allowed users can access certain content. A common approach is using **signed URLs or signed cookies** to serve private content. With a signed URL, the application generates a URL with an **expiry timestamp and an HMAC signature** (using a secret key). The CDN validates this signature on each request – if it’s missing or invalid (e.g. URL was tampered with, or expired), the CDN denies access. This allows time-limited, non-guessable links for protected content (videos, files, paywalled documents, etc.). Signed cookies work similarly: the app sets a cookie with a signed token that the CDN checks for each resource request, convenient for restricting access to a whole site/app after a user logs in. Under the hood, these tokens often use **HMAC (Hash-based Message Authentication Codes)** – essentially, a secret-key signature that the CDN verifies matches the URL/path and expiration. Cloudflare, for example, provides a built-in function to validate timed HMAC tokens on requests. Token-based auth can also integrate with JWTs or OIDC identity tokens. The CDN can be configured (or via edge compute like Lambda\@Edge/Cloudflare Workers) to verify a JWT’s signature and claims (like a scopes or user ID) before serving content.

Another aspect is **hot-link protection**. This ensures other sites cannot steal your content by directly embedding your CDN URLs. The CDN can check the **Referer header** of requests – if the request isn’t coming from an allowed domain (or has no referer), it will be blocked. For instance, Cloudflare’s Hotlink Protection will only allow requests where the HTTP referer matches your website, preventing random external sites from displaying your images. This stops bandwidth theft where an attacker hosts your images/media on their site via direct CDN links. Similarly, headers like `Origin` or custom tokens can be used to enforce that only your web pages can load certain resources. Many CDNs also support setting **authorization headers** or tokens to origin. For example, you could use a Lambda\@Edge function to inject an `Authorization` header on the origin request and have your origin verify it – meaning even if someone knew an origin URL, they couldn’t fetch it without going through the CDN that adds the secret. In summary, through signed URLs/cookies and header-based checks, the CDN ensures **only authorized users and sites** get the content. These methods are often combined: e.g. a user logs in and gets a signed cookie (with HMAC token) granting access for an hour of video streaming, and the CDN validates that token on each segment request to keep the stream private to that user.

## Observability Stack and Golden Signals

Operating a CDN at scale requires extensive **observability** of both the edge and origin performance. CDNs monitor the four *golden signals* – latency, traffic, errors, and saturation – across their infrastructure. Key **latency metrics** include the **edge RTT** (round-trip time from users to the edge PoP) as well as **origin latency** (time from the edge to fetch from origin on cache miss). A rise in edge RTT in a region could indicate network issues or an overloaded PoP, while high origin latency could mean origin server slowness. **Traffic** is monitored in requests per second, bandwidth, and connections to see usage patterns. **Errors** are tracked via HTTP status code buckets: notably 4xx (client errors like auth failures or missing content) and 5xx (server/proxy errors). Spikes in 5xx error rates at a given PoP might signal an internal failure, triggering an alert. The CDN also closely watches **cache hit vs miss ratio** as a vital performance indicator – the percentage of requests served from cache versus going to origin. A high cache hit ratio (e.g. 90%+) means low origin load and quick responses, whereas a falling hit ratio means more misses, higher origin latency, and potential strain on the origin. Other custom metrics include **connection churn**, TLS handshake times, and **PoP saturation** metrics. Saturation can refer to infrastructure capacity – e.g. CPU or memory usage on edge servers, or network interface utilization. If a PoP is near saturation (high CPU or bandwidth), latency may increase and the CDN may proactively shift new traffic elsewhere. Many CDNs expose **real-time logs** and metrics streaming. For example, **CloudFront real-time logs** can emit each request’s details within seconds, allowing building live dashboards of metrics like response times, bytes served, cache hit/miss, etc. These feed into monitoring systems (Datadog, Grafana, etc.) to watch the CDN’s health globally. Operators often have **dashboards per region/PoP** showing golden signals: e.g. a map of the world with current latency, request rates, error rates, and cache efficiency per region. **PoP saturation** is watched via system metrics and queue lengths; if a PoP is overload, it might be marked “degraded” in the dashboard. All this telemetry helps SREs quickly identify issues (like “Europe North PoP experiencing 5x error spike and high CPU”) and pinpoint problems before they impact many users.

## Alerting and SLOs

With rich metrics in place, CDNs implement sophisticated **alerting tied to SLOs (Service Level Objectives)**. Rather than alert on every blip, SRE teams define SLOs for key metrics – e.g. “99.9% of requests have < 200ms edge latency” or “99.99% availability (<=0.01% error rate) per month”. **Error budgets** (the allowable amount of errors or latency breaches) are used to manage alerts. A modern approach is **burn-rate alerts**: this looks at how fast the CDN is consuming its error budget. For example, if the SLO is 99.9% (meaning 0.1% errors allowed), a constant 0.2% error rate would burn through the monthly budget twice as fast (burn rate = 2.0). Alerts can be set to trigger if the error budget burn rate is too high over a short window – meaning things are on track to violate the SLO long before the period ends. This catches incidents quickly. A common strategy is multi-window, multi-burn-rate alerting: e.g. page the team if the 1-hour error budget burn rate > 14 (meaning massive outage likely) or if the 6-hour burn rate > 4, etc., as recommended by SRE practices. Similarly, **latency SLOs** can have budgets – e.g. 95th percentile latency must be under 300ms for 99% of requests. If latency spikes, a “fast burn” on that budget triggers an alert. Aside from SLO-based alerts, CDNs still use threshold alerts for certain signals: sudden drops in cache hit ratio or availability will fire alerts. For example, an alert might trigger if *global 5xx errors exceed 0.5% for 5 minutes* or *any single region’s error rate >5%*. To avoid alert fatigue, teams tune these with **warning vs critical levels** and use **alert grouping**. A major outage can cause dozens of symptom alerts (latency, errors across many PoPs), so systems group them into one incident notification. **Dashboards and runbooks** are then used to diagnose the issue. Composite dashboards often overlay multiple data sources – for instance, an SLO dashboard might show the error rate vs budget for each service, combined with traffic and latency charts, to give a holistic view. In an interview context, it’s good to mention using **burn-rate alerts to catch outages early without being too noisy**, and having **error budgets** to balance reliability and feature velocity. CDNs also integrate with on-call systems (PagerDuty, etc.) to ensure the right people get alerted when SLOs are in danger, using techniques like **budget burn alerts** rather than just raw metrics.

## Capacity Levers and Surge Control

A CDN must handle sudden traffic spikes (flash crowds or surges) gracefully. **Capacity levers** are strategies to avoid overload by dynamically adjusting how traffic is handled. One tactic is maintaining **warm-spare PoPs** or servers – essentially having some capacity on standby. For example, if one region’s PoPs are at 80% load, the CDN can activate additional caches (already running and “warm” with content) to share the load, or route some traffic to a nearby region with headroom. CDNs also use **surge queues** at load balancers to buffer bursts. If traffic spikes beyond what the origin or cluster can handle instantly, the load balancer will queue some requests for a short time instead of dropping them. However, these queues have limits – if the **surge queue fills up**, the system starts shedding load. In AWS terminology, when the surge queue is full, the ELB returns HTTP 503 errors (or TCP resets) to excess requests. This is essentially an **admission control** mechanism: beyond a certain point, the CDN will prefer to reject new requests rather than overwhelm the servers completely. By failing fast with an error or “please try later” response, the system protects the core service. Another lever related to multi-layer caching is **origin shielding** (which some providers call **shield**). For instance, AWS CloudFront’s Origin Shield designates a particular PoP or region to be a caching layer in front of the origin. This reduces origin load by consolidating cache misses. If that shield layer gets overwhelmed (too many cache misses), the CDN might temporarily bypass the shield (spill-over) and have some edges fetch directly from origin to distribute load. In other words, **shield spill-over** is allowing secondary paths when the primary shield is saturated, to avoid a single bottleneck. Additionally, **admission control** policies can include dropping or throttling *non-critical traffic* first. For example, during an extreme surge, a CDN might intentionally downgrade video quality or delay non-essential asset loads to prioritize critical content delivery. Some CDN architectures employ **capacity reservations** for premium customers – ensuring even under extreme load, paying tenants have reserved throughput while best-effort traffic might get slower responses. Cloud providers like AWS provide **autoscaling** for origin fleets behind a CDN, but at the CDN layer itself, capacity management is about intelligently using the global network. **Load shedding**, **queued requests**, and **burst absorbing** (via short-term caching of even dynamic errors) all come into play to handle surges. In summary, the CDN’s design includes safety valves: *when load spikes*, add more cache nodes if possible, queue some requests briefly, and if things still exceed limits, **fail gracefully** (serve errors or static “busy” responses) rather than collapsing under the load. This ensures partial availability (some requests succeed) and quicker recovery once the surge passes.

## AWS Reference Touchpoints

*While the principles above are vendor-agnostic, it helps to relate them to real services for context.* In AWS’s ecosystem, many of these capabilities are provided by specific services or features. For example, **AWS WAF** (Web Application Firewall) is used to deploy the rule-based protections and bot mitigations at the CDN edge – including managed rule sets and custom rules for things like SQL injection, IP blocklists, or rate limiting. AWS WAF recently added **JA3 fingerprint** matching, illustrating the advanced bot filtering now available (similar to Cloudflare’s Bot Management). On the DDoS front, AWS’s **Shield Advanced** service integrates with CloudFront to provide always-on detection and automated mitigation for large L3/L4 attacks, with the benefit of AWS’s global network capacity. For transport security, Amazon CloudFront supports TLS 1.3 and even allows uploading custom SSL certificates or using AWS Certificate Manager for free cert provisioning. It also supports **OCSP stapling and HSTS** preloading for better security. CloudFront can be configured with **Origin Access Identity** or **mTLS to origin** (e.g. by requiring CloudFront to present a client certificate to your load balancer/origin), ensuring the origin only accepts traffic from CloudFront. For content authorization, CloudFront has **signed URL and signed cookie** features out-of-the-box – you’d generate signed URLs via AWS SDKs, and CloudFront will enforce expiration and signature checks. If those don’t meet a specific need, you can use **Lambda\@Edge** functions to implement custom auth logic. For instance, a Lambda\@Edge trigger on viewer request can read a JWT from a cookie and allow or deny based on its claims (this is a common pattern to implement authentication at the edge without hitting the origin). CloudFront also provides **real-time logs** and standard logs that can feed into an observability stack – AWS Kinesis Firehose can stream real-time logs to analysis systems for building dashboards. Metrics wise, CloudFront is integrated with CloudWatch, exposing metrics like `4xxErrorRate`, `5xxErrorRate`, `OriginLatency`, `CacheHitRate`, and even `CapacityUsed` per edge location. These tie into constructing SLOs; AWS has **CloudWatch SLO/alarm** capabilities and even an Error Budget monitoring solution. For surge and capacity control, AWS’s ethos is often to scale out – but CloudFront does have internal capacity management. When using **Elastic Load Balancing** behind CloudFront, monitoring the **SurgeQueueLength** and **SpilloverCount** metrics on the ELB is critical. A high SpilloverCount (requests dropped due to full queue) indicates the origin can’t keep up, and you may need to scale it or cache more aggressively. In summary, AWS provides a service or feature for each concept in CDN design: AWS WAF for WAF, Shield Advanced for DDoS, CloudFront logs and CloudWatch for observability, and Lambda\@Edge for extending logic at the edge. Understanding these concrete tools can help illustrate the general design approaches in a system design interview, without making the answer purely about AWS. Each CDN provider will have analogous features, but the underlying principles of resilience, security, and observability remain the same across vendors.
